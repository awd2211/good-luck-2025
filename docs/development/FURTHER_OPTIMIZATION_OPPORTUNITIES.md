# è¿›ä¸€æ­¥ä¼˜åŒ–æœºä¼šåˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: 2025-11-16
**åˆ†æèŒƒå›´**: æ•°æ®åº“ã€åç«¯ä»£ç ã€æ¶æ„
**åŸºäº**: DATABASE_OPTIMIZATION_COMPLETED.md å®Œæˆåçš„æ·±åº¦åˆ†æ

---

## ğŸ”´ é«˜ä¼˜å…ˆçº§ä¼˜åŒ–ï¼ˆç«‹å³æ‰§è¡Œï¼‰

### 1. å¤–é”®ç´¢å¼•ç¼ºå¤± âš ï¸ **ä¸¥é‡æ€§èƒ½é—®é¢˜**

**é—®é¢˜**: å‘ç° **17ä¸ªå¤–é”®åˆ—æ²¡æœ‰ç´¢å¼•**ï¼Œè¿™ä¼šå¯¼è‡´JOINæŸ¥è¯¢å’Œçº§è”åˆ é™¤æ“ä½œææ…¢ã€‚

**å½±å“çš„è¡¨å’Œåˆ—**:
```sql
ai_conversation_logs.bot_config_id
attribution_touchpoints.attribution_event_id
attribution_touchpoints.channel_id
attribution_utm_templates.channel_id
customer_profiles.preferred_agent_id
fortune_results.fortune_id
invite_records.share_link_id
knowledge_search_history.clicked_article_id
notifications.template_id
order_items.fortune_result_id
promotion_codes.channel_id
schedule_swap_requests.requester_schedule_id
schedule_swap_requests.target_schedule_id
share_conversions.click_id
share_rewards.conversion_id
share_rewards.share_link_id
user_tags.assigned_by
```

**é¢„æœŸå½±å“**:
- JOINæŸ¥è¯¢å¯èƒ½æ…¢ **10-100å€**
- çº§è”åˆ é™¤æ“ä½œå¯èƒ½å¯¼è‡´é”è¡¨

**è§£å†³æ–¹æ¡ˆ**: åˆ›å»ºç´¢å¼•

```sql
-- åˆ›å»ºå¤–é”®ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_ai_conversation_logs_bot_config_id ON ai_conversation_logs(bot_config_id);
CREATE INDEX CONCURRENTLY idx_attribution_touchpoints_event_id ON attribution_touchpoints(attribution_event_id);
CREATE INDEX CONCURRENTLY idx_attribution_touchpoints_channel_id ON attribution_touchpoints(channel_id);
CREATE INDEX CONCURRENTLY idx_attribution_utm_templates_channel_id ON attribution_utm_templates(channel_id);
CREATE INDEX CONCURRENTLY idx_customer_profiles_preferred_agent ON customer_profiles(preferred_agent_id);
CREATE INDEX CONCURRENTLY idx_fortune_results_fortune_id ON fortune_results(fortune_id);
CREATE INDEX CONCURRENTLY idx_invite_records_share_link_id ON invite_records(share_link_id);
CREATE INDEX CONCURRENTLY idx_knowledge_search_clicked_article ON knowledge_search_history(clicked_article_id);
CREATE INDEX CONCURRENTLY idx_notifications_template_id ON notifications(template_id);
CREATE INDEX CONCURRENTLY idx_order_items_fortune_result_id ON order_items(fortune_result_id);
CREATE INDEX CONCURRENTLY idx_promotion_codes_channel_id ON promotion_codes(channel_id);
CREATE INDEX CONCURRENTLY idx_schedule_swap_requester ON schedule_swap_requests(requester_schedule_id);
CREATE INDEX CONCURRENTLY idx_schedule_swap_target ON schedule_swap_requests(target_schedule_id);
CREATE INDEX CONCURRENTLY idx_share_conversions_click_id ON share_conversions(click_id);
CREATE INDEX CONCURRENTLY idx_share_rewards_conversion_id ON share_rewards(conversion_id);
CREATE INDEX CONCURRENTLY idx_share_rewards_share_link_id ON share_rewards(share_link_id);
CREATE INDEX CONCURRENTLY idx_user_tags_assigned_by ON user_tags(assigned_by);
```

**é¢„æœŸæ”¶ç›Š**: JOINæŸ¥è¯¢æ€§èƒ½æå‡ 10-100å€

---

### 2. è¡¨è†¨èƒ€æ¸…ç† âš ï¸ **å½±å“æ€§èƒ½å’Œç£ç›˜ç©ºé—´**

**é—®é¢˜**: å¤šä¸ªè¡¨æœ‰å¤§é‡æ­»å…ƒç»„ï¼ˆdead tuplesï¼‰ï¼Œå ç”¨ç©ºé—´ä¸”å½±å“æŸ¥è¯¢æ€§èƒ½ã€‚

**ä¸¥é‡è†¨èƒ€çš„è¡¨**:

| è¡¨å | æ´»å…ƒç»„ | æ­»å…ƒç»„ | è†¨èƒ€ç‡ | å½±å“ |
|------|-------|--------|--------|------|
| `payments` | 1 | 5 | **83.33%** | æé«˜ |
| `cart_items` | 1 | 3 | **75.00%** | é«˜ |
| `notifications` | 3 | 5 | **62.50%** | é«˜ |
| `user_coupons` | 2 | 3 | **60.00%** | ä¸­ |
| `admins` | 8 | 11 | **57.89%** | ä¸­ |
| `orders` | 7 | 8 | **53.33%** | ä¸­ |

**è§£å†³æ–¹æ¡ˆ**: æ‰§è¡ŒVACUUM

```bash
# ç«‹å³æ‰§è¡Œå®Œæ•´VACUUM ANALYZE
docker exec fortune-postgres psql -U fortune_user -d fortune_db -c "VACUUM ANALYZE;"

# å¯¹ä¸¥é‡è†¨èƒ€çš„è¡¨æ‰§è¡ŒFULL VACUUMï¼ˆä¼šé”è¡¨ï¼Œé€‰æ‹©ä½å³°æœŸæ‰§è¡Œï¼‰
docker exec fortune-postgres psql -U fortune_user -d fortune_db -c "VACUUM FULL ANALYZE payments;"
docker exec fortune-postgres psql -U fortune_user -d fortune_db -c "VACUUM FULL ANALYZE cart_items;"
docker exec fortune-postgres psql -U fortune_user -d fortune_db -c "VACUUM FULL ANALYZE notifications;"
```

**è‡ªåŠ¨VACUUMé…ç½®**ï¼ˆå½“å‰å·²å¯ç”¨ï¼Œä½†å¯ä»¥ä¼˜åŒ–ï¼‰:
```sql
-- å¯¹é«˜é¢‘æ›´æ–°çš„è¡¨è°ƒæ•´è‡ªåŠ¨VACUUMè§¦å‘é˜ˆå€¼
ALTER TABLE payments SET (autovacuum_vacuum_threshold = 25);
ALTER TABLE cart_items SET (autovacuum_vacuum_threshold = 25);
ALTER TABLE notifications SET (autovacuum_vacuum_threshold = 25);
```

**é¢„æœŸæ”¶ç›Š**:
- æŸ¥è¯¢æ€§èƒ½æå‡ 20-50%
- ç£ç›˜ç©ºé—´å›æ”¶ ~50%

---

### 3. å¯ç”¨æ…¢æŸ¥è¯¢è¿½è¸ª âš ï¸ **å…³é”®ç›‘æ§å·¥å…·**

**é—®é¢˜**: `pg_stat_statements` æ‰©å±•æœªå¯ç”¨ï¼Œæ— æ³•è¿½è¸ªå’Œä¼˜åŒ–æ…¢æŸ¥è¯¢ã€‚

**è§£å†³æ–¹æ¡ˆ**:

```sql
-- 1. å¯ç”¨æ‰©å±•
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- 2. æŸ¥çœ‹æœ€æ…¢çš„10ä¸ªæŸ¥è¯¢
SELECT
    query,
    calls,
    ROUND(mean_exec_time::numeric, 2) as avg_ms,
    ROUND(max_exec_time::numeric, 2) as max_ms,
    ROUND(total_exec_time::numeric, 2) as total_ms
FROM pg_stat_statements
WHERE mean_exec_time > 10
ORDER BY mean_exec_time DESC
LIMIT 10;

-- 3. æŸ¥çœ‹æœ€é¢‘ç¹çš„æŸ¥è¯¢
SELECT
    query,
    calls,
    ROUND(mean_exec_time::numeric, 2) as avg_ms
FROM pg_stat_statements
ORDER BY calls DESC
LIMIT 10;
```

**æŒä¹…åŒ–é…ç½®**ï¼ˆéœ€è¦ä¿®æ”¹ PostgreSQL é…ç½®ï¼‰:
```
# åœ¨ postgresql.conf ä¸­æ·»åŠ 
shared_preload_libraries = 'pg_stat_statements'
pg_stat_statements.max = 10000
pg_stat_statements.track = all
```

**é¢„æœŸæ”¶ç›Š**:
- è¯†åˆ«çœŸå®çš„æ€§èƒ½ç“¶é¢ˆ
- æ•°æ®é©±åŠ¨çš„ä¼˜åŒ–å†³ç­–

---

## ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ä¼˜åŒ–ï¼ˆ1-2å‘¨å†…æ‰§è¡Œï¼‰

### 4. Redisç¼“å­˜ä½¿ç”¨ä¸è¶³

**é—®é¢˜**: Rediså·²å¯ç”¨ä½†ä½¿ç”¨ç‡ä½ï¼Œå¤§é‡é‡å¤æŸ¥è¯¢æœªç¼“å­˜ã€‚

**å½“å‰çŠ¶æ€**:
- Rediså®¹å™¨è¿è¡Œ: `fortune-redis` (ç«¯å£ 6380)
- é…ç½®: `REDIS_ENABLED=true`
- å®é™…ä½¿ç”¨: ä»…éƒ¨åˆ†fortune APIæœ‰ç¼“å­˜

**ä¼˜åŒ–å»ºè®®**:

#### A. ç”¨æˆ·ä¿¡æ¯ç¼“å­˜
```typescript
// backend/src/services/manage/userService.ts
import { redisCache } from '../../config/redis';

export async function getUserById(id: string) {
  // å°è¯•ä»ç¼“å­˜è·å–
  const cacheKey = `user:${id}`;
  const cached = await redisCache.get<User>(cacheKey);
  if (cached) return cached;

  // ä»æ•°æ®åº“æŸ¥è¯¢
  const query = `...`;
  const result = await pool.query(query, [id]);

  if (result.rows.length > 0) {
    // ç¼“å­˜30åˆ†é’Ÿ
    await redisCache.set(cacheKey, result.rows[0], 1800);
  }

  return result.rows[0];
}
```

#### B. è®¢å•åˆ—è¡¨ç¼“å­˜
```typescript
// backend/src/services/user/orderService.ts
export async function getUserOrders(userId: string, page: number) {
  const cacheKey = `orders:${userId}:${page}`;
  const cached = await redisCache.get<UserOrder[]>(cacheKey);
  if (cached) return cached;

  // æŸ¥è¯¢æ•°æ®åº“...
  const orders = await query(...);

  // ç¼“å­˜5åˆ†é’Ÿ
  await redisCache.set(cacheKey, orders, 300);
  return orders;
}
```

#### C. çƒ­é—¨æ•°æ®ç¼“å­˜
```typescript
// ç¼“å­˜ç­–ç•¥
const cacheStrategies = {
  'fortune-list': { ttl: 3600 },      // 1å°æ—¶
  'user-info': { ttl: 1800 },         // 30åˆ†é’Ÿ
  'order-list': { ttl: 300 },         // 5åˆ†é’Ÿ
  'cart': { ttl: 600 },               // 10åˆ†é’Ÿ
  'stats': { ttl: 1800 },             // 30åˆ†é’Ÿ
};
```

**é¢„æœŸæ”¶ç›Š**:
- APIå“åº”æ—¶é—´å‡å°‘ 80-90%
- æ•°æ®åº“è´Ÿè½½å‡å°‘ 60-70%

---

### 5. N+1æŸ¥è¯¢ä¼˜åŒ–

**é—®é¢˜**: å‘ç°10ä¸ªæ–‡ä»¶å­˜åœ¨å¾ªç¯ä¸­çš„æ•°æ®åº“æŸ¥è¯¢ã€‚

**å·²å‘ç°çš„æ½œåœ¨N+1æŸ¥è¯¢**:
```
src/services/webchat/trainingService.ts
src/services/webchat/csScheduleService.ts
src/services/webchat/sensitiveWordService.ts
src/services/configService.ts
src/services/webchat/customerTagService.ts
src/services/webchat/statisticsService.ts
src/services/twoFactorService.ts
src/services/aiService.ts
src/services/notificationScheduler.ts
src/services/user/orderService.ts
```

**ç¤ºä¾‹é—®é¢˜**ï¼ˆuser/orderService.tsï¼‰:
```typescript
// âŒ é—®é¢˜ä»£ç ï¼ˆN+1æŸ¥è¯¢ï¼‰
for (const item of items) {
  const fortune = await query('SELECT * FROM fortunes WHERE id = $1', [item.fortuneId]);
  // æ¯ä¸ªiteméƒ½æŸ¥è¯¢ä¸€æ¬¡æ•°æ®åº“
}

// âœ… ä¼˜åŒ–æ–¹æ¡ˆï¼ˆæ‰¹é‡æŸ¥è¯¢ï¼‰
const fortuneIds = items.map(item => item.fortuneId);
const fortunes = await query(
  'SELECT * FROM fortunes WHERE id = ANY($1)',
  [fortuneIds]
);
const fortuneMap = new Map(fortunes.rows.map(f => [f.id, f]));
for (const item of items) {
  const fortune = fortuneMap.get(item.fortuneId);
}
```

**éœ€è¦å®¡æŸ¥çš„ä»£ç æ¨¡å¼**:
```bash
grep -rn "for.*await.*query" src/services/
grep -rn "\.map.*async.*pool\.query" src/services/
```

**é¢„æœŸæ”¶ç›Š**:
- æ‰¹é‡æ“ä½œæ€§èƒ½æå‡ 10-50å€
- å‡å°‘æ•°æ®åº“è¿æ¥å¼€é”€

---

### 6. æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–

**å½“å‰é…ç½®**:
```typescript
poolMax: 10,               // æœ€å¤§è¿æ¥æ•°
poolMin: 2,                // æœ€å°è¿æ¥æ•°
idleTimeoutMillis: 30000,  // ç©ºé—²è¶…æ—¶
```

**å®é™…ä½¿ç”¨**: 7/100 æ•°æ®åº“è¿æ¥ (PostgreSQL max_connections=100)

**ä¼˜åŒ–å»ºè®®**:

#### A. æ ¹æ®è´Ÿè½½è°ƒæ•´è¿æ¥æ± 
```typescript
// backend/src/config/index.ts
database: {
  poolMax: parseInt(optional('DB_POOL_MAX', '20')),      // å¢åŠ åˆ°20
  poolMin: parseInt(optional('DB_POOL_MIN', '5')),       // å¢åŠ æœ€å°å€¼
  idleTimeoutMillis: parseInt(optional('DB_IDLE_TIMEOUT', '10000')), // å‡å°‘åˆ°10s
  connectionTimeoutMillis: parseInt(optional('DB_CONNECTION_TIMEOUT', '5000')),

  // æ–°å¢é…ç½®
  statement_timeout: parseInt(optional('DB_STATEMENT_TIMEOUT', '30000')), // 30sæŸ¥è¯¢è¶…æ—¶
}
```

#### B. æ·»åŠ è¿æ¥æ± ç›‘æ§
```typescript
// backend/src/config/database.ts
pool.on('acquire', () => {
  console.log('ğŸ“Š è¿æ¥æ± çŠ¶æ€:', {
    total: pool.totalCount,
    idle: pool.idleCount,
    waiting: pool.waitingCount,
  });
});

pool.on('remove', () => {
  console.log('âš ï¸ è¿æ¥è¢«ç§»é™¤');
});
```

**é¢„æœŸæ”¶ç›Š**:
- é«˜å¹¶å‘ä¸‹è¿æ¥ç­‰å¾…æ—¶é—´å‡å°‘
- æ›´å¥½çš„èµ„æºåˆ©ç”¨ç‡

---

## ğŸŸ¢ ä½ä¼˜å…ˆçº§ä¼˜åŒ–ï¼ˆé•¿æœŸè§„åˆ’ï¼‰

### 7. audit_logsè¡¨åˆ†åŒº

**é—®é¢˜**: `audit_logs` è¡¨ 2060è¡Œï¼Œ1.2MBï¼Œé¢„è®¡ä¼šå¿«é€Ÿå¢é•¿ã€‚

**è§£å†³æ–¹æ¡ˆ**: æŒ‰æœˆåˆ†åŒº

```sql
-- 1. åˆ›å»ºåˆ†åŒºçˆ¶è¡¨
CREATE TABLE audit_logs_partitioned (
    id SERIAL,
    action VARCHAR(50),
    admin_id VARCHAR(50),
    target_type VARCHAR(50),
    target_id VARCHAR(100),
    changes JSONB,
    ip_address VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
) PARTITION BY RANGE (created_at);

-- 2. åˆ›å»ºæœˆåº¦åˆ†åŒº
CREATE TABLE audit_logs_2025_01 PARTITION OF audit_logs_partitioned
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

CREATE TABLE audit_logs_2025_02 PARTITION OF audit_logs_partitioned
    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');

-- 3. åˆ›å»ºç´¢å¼•
CREATE INDEX idx_audit_logs_2025_01_created ON audit_logs_2025_01(created_at);
CREATE INDEX idx_audit_logs_2025_01_admin ON audit_logs_2025_01(admin_id);

-- 4. è¿ç§»ç°æœ‰æ•°æ®
INSERT INTO audit_logs_partitioned SELECT * FROM audit_logs;

-- 5. è‡ªåŠ¨åˆ›å»ºæœªæ¥åˆ†åŒºï¼ˆä½¿ç”¨pg_cronæˆ–åº”ç”¨å±‚ï¼‰
```

**é¢„æœŸæ”¶ç›Š**:
- æŸ¥è¯¢æ€§èƒ½ä¿æŒç¨³å®šï¼ˆå³ä½¿ç™¾ä¸‡çº§æ•°æ®ï¼‰
- æ—§æ•°æ®å½’æ¡£æ›´å®¹æ˜“

---

### 8. å®æ–½å¤šå±‚ç¼“å­˜ç­–ç•¥

**æ¶æ„**:
```
ç”¨æˆ·è¯·æ±‚
    â†“
[L1: åº”ç”¨å†…å­˜ç¼“å­˜] (1åˆ†é’Ÿ, LRU 1000æ¡)
    â†“ miss
[L2: Redisç¼“å­˜] (10åˆ†é’Ÿ)
    â†“ miss
[L3: æ•°æ®åº“]
```

**å®ç°ç¤ºä¾‹**:
```typescript
// backend/src/middleware/multiLevelCache.ts
import NodeCache from 'node-cache';
import { redisCache } from '../config/redis';

const l1Cache = new NodeCache({
  stdTTL: 60,           // 1åˆ†é’Ÿ
  checkperiod: 120,
  maxKeys: 1000,
  useClones: false      // æ€§èƒ½ä¼˜åŒ–
});

export async function getWithCache<T>(
  key: string,
  fetcher: () => Promise<T>,
  l2TTL: number = 600   // Redis 10åˆ†é’Ÿ
): Promise<T> {
  // L1: å†…å­˜ç¼“å­˜
  let data = l1Cache.get<T>(key);
  if (data) {
    console.log('âœ… L1ç¼“å­˜å‘½ä¸­:', key);
    return data;
  }

  // L2: Redisç¼“å­˜
  data = await redisCache.get<T>(key);
  if (data) {
    console.log('âœ… L2ç¼“å­˜å‘½ä¸­:', key);
    l1Cache.set(key, data);
    return data;
  }

  // L3: æ•°æ®åº“
  console.log('âš ï¸ ç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“:', key);
  data = await fetcher();

  // å†™å…¥ç¼“å­˜
  await redisCache.set(key, data, l2TTL);
  l1Cache.set(key, data);

  return data;
}
```

**é¢„æœŸæ”¶ç›Š**:
- å†…å­˜ç¼“å­˜å‘½ä¸­: **1-2ms å“åº”**
- Redisç¼“å­˜å‘½ä¸­: **5-10ms å“åº”**
- æ•°æ®åº“æŸ¥è¯¢: **50-200ms å“åº”**

---

### 9. è¯»å†™åˆ†ç¦»æ¶æ„

**å½“å‰æ¶æ„**: å•ä¸€PostgreSQLä¸»åº“

**ä¼˜åŒ–æ–¹æ¡ˆ**: ä¸€ä¸»ä¸€ä»è¯»å†™åˆ†ç¦»

```
å†™æ“ä½œ â†’ [PostgreSQL Master]
            â†“ æµå¤åˆ¶
è¯»æ“ä½œ â†’ [PostgreSQL Replica]
```

**å®ç°æ­¥éª¤**:
1. é…ç½®PostgreSQLæµå¤åˆ¶
2. ä¿®æ”¹æ•°æ®åº“è¿æ¥æ± 
3. è·¯ç”±è§„åˆ™: å†™å…¥èµ°masterï¼ŒæŸ¥è¯¢èµ°replica

```typescript
// backend/src/config/database.ts
const masterPool = new Pool({ ... });  // å†™æ“ä½œ
const replicaPool = new Pool({ ... }); // è¯»æ“ä½œ

export const write = async (text: string, params?: any[]) => {
  return masterPool.query(text, params);
};

export const read = async (text: string, params?: any[]) => {
  return replicaPool.query(text, params);
};
```

**é¢„æœŸæ”¶ç›Š**:
- è¯»æ€§èƒ½æå‡ 50-100%
- ä¸»åº“å†™å…¥å‹åŠ›å‡å°‘
- é«˜å¯ç”¨æ€§

---

## ğŸ“Š ä¼˜åŒ–ä¼˜å…ˆçº§çŸ©é˜µ

| ä¼˜åŒ–é¡¹ | å½±å“ | å®æ–½éš¾åº¦ | é¢„æœŸæ”¶ç›Š | ä¼˜å…ˆçº§ |
|--------|------|---------|---------|--------|
| å¤–é”®ç´¢å¼• | ğŸ”´ æé«˜ | â­ ç®€å• | 10-100å€ | **P0** |
| è¡¨è†¨èƒ€æ¸…ç† | ğŸ”´ é«˜ | â­ ç®€å• | 20-50% | **P0** |
| æ…¢æŸ¥è¯¢è¿½è¸ª | ğŸŸ¡ ä¸­ | â­ ç®€å• | ç›‘æ§èƒ½åŠ› | **P0** |
| Redisç¼“å­˜æ‰©å±• | ğŸ”´ é«˜ | â­â­ ä¸­ç­‰ | 80-90% | **P1** |
| N+1æŸ¥è¯¢ä¼˜åŒ– | ğŸŸ¡ ä¸­ | â­â­â­ å›°éš¾ | 10-50å€ | **P1** |
| è¿æ¥æ± ä¼˜åŒ– | ğŸŸ¢ ä½ | â­ ç®€å• | å¹¶å‘èƒ½åŠ› | **P2** |
| è¡¨åˆ†åŒº | ğŸŸ¢ ä½ | â­â­â­ å›°éš¾ | é•¿æœŸæ€§èƒ½ | **P3** |
| å¤šå±‚ç¼“å­˜ | ğŸŸ¡ ä¸­ | â­â­ ä¸­ç­‰ | æè‡´æ€§èƒ½ | **P3** |
| è¯»å†™åˆ†ç¦» | ğŸŸ¢ ä½ | â­â­â­â­ å¾ˆéš¾ | 50-100% | **P4** |

---

## ğŸ¯ æ¨èæ‰§è¡Œè®¡åˆ’

### ç¬¬1å‘¨ï¼ˆç«‹å³æ‰§è¡Œï¼‰
1. âœ… åˆ›å»º17ä¸ªå¤–é”®ç´¢å¼• (30åˆ†é’Ÿ)
2. âœ… æ‰§è¡ŒVACUUMæ¸…ç† (1å°æ—¶)
3. âœ… å¯ç”¨pg_stat_statements (10åˆ†é’Ÿ)
4. âœ… ç›‘æ§1å‘¨ï¼Œæ”¶é›†æ…¢æŸ¥è¯¢æ•°æ®

### ç¬¬2-3å‘¨
5. âœ… æ‰©å±•Redisç¼“å­˜ä½¿ç”¨ (2-3å¤©)
6. âœ… ä¿®å¤å·²è¯†åˆ«çš„N+1æŸ¥è¯¢ (3-5å¤©)
7. âœ… ä¼˜åŒ–æ•°æ®åº“è¿æ¥æ± é…ç½® (1å¤©)

### ç¬¬4å‘¨+
8. â³ æ ¹æ®ç›‘æ§æ•°æ®è¿›ä¸€æ­¥ä¼˜åŒ–
9. â³ è¯„ä¼°è¡¨åˆ†åŒºéœ€æ±‚
10. â³ è€ƒè™‘å¤šå±‚ç¼“å­˜æ¶æ„

---

## ğŸ“ˆ é¢„æœŸæ•´ä½“æ”¶ç›Š

**ç«‹å³ä¼˜åŒ–å**ï¼ˆç¬¬1å‘¨ï¼‰:
- JOINæŸ¥è¯¢: **10-100å€ æå‡**
- æ•´ä½“æŸ¥è¯¢: **30-50% æå‡**
- ç£ç›˜ç©ºé—´: **å›æ”¶50%**

**å®Œæ•´ä¼˜åŒ–å**ï¼ˆç¬¬3å‘¨ï¼‰:
- APIå“åº”æ—¶é—´: **80-90% å‡å°‘**
- æ•°æ®åº“è´Ÿè½½: **60-70% å‡å°‘**
- å¹¶å‘èƒ½åŠ›: **2-3å€ æå‡**

**é•¿æœŸä¼˜åŒ–å**ï¼ˆ3ä¸ªæœˆ+ï¼‰:
- ç³»ç»Ÿæ”¯æŒ **10å€ä»¥ä¸Š** çš„ç”¨æˆ·é‡
- æŸ¥è¯¢æ€§èƒ½ä¿æŒç¨³å®šï¼ˆå³ä½¿æ•°æ®é‡å¢é•¿100å€ï¼‰
- é«˜å¯ç”¨æ€§å’Œå¯æ‰©å±•æ€§

---

## ğŸ› ï¸ å¿«é€Ÿå¼€å§‹

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ç«‹å³è·å¾—30-50%çš„æ€§èƒ½æå‡ï¼š

```bash
# 1. åˆ›å»ºå¤–é”®ç´¢å¼•è¿ç§»æ–‡ä»¶
cat > backend/migrations/021_foreign_key_indexes.sql << 'EOF'
-- å¤–é”®ç´¢å¼•ä¼˜åŒ–
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_ai_conversation_logs_bot_config_id ON ai_conversation_logs(bot_config_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_attribution_touchpoints_event_id ON attribution_touchpoints(attribution_event_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_attribution_touchpoints_channel_id ON attribution_touchpoints(channel_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_attribution_utm_templates_channel_id ON attribution_utm_templates(channel_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_customer_profiles_preferred_agent ON customer_profiles(preferred_agent_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_fortune_results_fortune_id ON fortune_results(fortune_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_invite_records_share_link_id ON invite_records(share_link_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_knowledge_search_clicked_article ON knowledge_search_history(clicked_article_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_notifications_template_id ON notifications(template_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_order_items_fortune_result_id ON order_items(fortune_result_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_promotion_codes_channel_id ON promotion_codes(channel_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_schedule_swap_requester ON schedule_swap_requests(requester_schedule_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_schedule_swap_target ON schedule_swap_requests(target_schedule_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_share_conversions_click_id ON share_conversions(click_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_share_rewards_conversion_id ON share_rewards(conversion_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_share_rewards_share_link_id ON share_rewards(share_link_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_user_tags_assigned_by ON user_tags(assigned_by);

-- å¯ç”¨æ…¢æŸ¥è¯¢è¿½è¸ª
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;
EOF

# 2. æ‰§è¡Œè¿ç§»
docker exec -i fortune-postgres psql -U fortune_user -d fortune_db < backend/migrations/021_foreign_key_indexes.sql

# 3. æ‰§è¡ŒVACUUM
docker exec fortune-postgres psql -U fortune_user -d fortune_db -c "VACUUM ANALYZE;"

# 4. æŸ¥çœ‹ç´¢å¼•åˆ›å»ºè¿›åº¦ï¼ˆCONCURRENTLYæ˜¯éé˜»å¡çš„ï¼‰
docker exec fortune-postgres psql -U fortune_user -d fortune_db -c "
SELECT
    now()::time as current_time,
    query,
    state,
    wait_event_type,
    wait_event
FROM pg_stat_activity
WHERE query LIKE '%CREATE INDEX%';"
```

**å®Œæˆï¼** ğŸ‰

---

## ğŸ“ ç»´æŠ¤å»ºè®®

### æ¯å‘¨æ£€æŸ¥
```bash
# æ£€æŸ¥è¡¨è†¨èƒ€
docker exec fortune-postgres psql -U fortune_user -d fortune_db -c "
SELECT relname, n_dead_tup, n_live_tup,
       ROUND(100.0 * n_dead_tup / NULLIF(n_live_tup + n_dead_tup, 0), 2) as dead_pct
FROM pg_stat_user_tables
WHERE n_dead_tup > 100
ORDER BY n_dead_tup DESC;"
```

### æ¯æœˆæ£€æŸ¥
```bash
# æŸ¥çœ‹æ…¢æŸ¥è¯¢
docker exec fortune-postgres psql -U fortune_user -d fortune_db -c "
SELECT query, calls, mean_exec_time, max_exec_time
FROM pg_stat_statements
WHERE mean_exec_time > 100
ORDER BY mean_exec_time DESC
LIMIT 10;"

# æŸ¥çœ‹ç´¢å¼•ä½¿ç”¨ç‡
docker exec fortune-postgres psql -U fortune_user -d fortune_db -c "
SELECT
    schemaname, tablename, indexname,
    idx_scan, idx_tup_read, idx_tup_fetch
FROM pg_stat_user_indexes
WHERE idx_scan = 0
  AND indexname NOT LIKE 'pg_%'
ORDER BY schemaname, tablename;"
```

---

**æŠ¥å‘Šå®Œæˆ** âœ…
